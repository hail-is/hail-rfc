================================
Fast Restarts for Failed Queries
================================

.. author:: Edmund Higham <edhigham@gmail.com>
.. date-accepted:: Leave blank. This will be filled in when the proposal is accepted.
.. implemented:: Leave blank. This will be filled in with the first Hail version which
                 implements the described feature.
.. header:: This proposal is `discussed at this pull request <https://github.com/hail-is/hail-rfc/pull/0>`_.
            **After creating the pull request, edit this file again, update the
            number in the link, and delete this bold sentence.**
.. sectnum::
.. contents::
.. role:: scala(code)

.. Here you should write a short abstract motivating and briefly summarizing the
.. proposed change.

Motivation
==========
Evaluating a query involves executing a series of dependent parallel
computations; a single failure in any of these will result in the whole
query failing.
These failures can happen for a multitude of reasons - from programmer error to
general cloud instability.
Sometimes, the only way to fix a failure is to re-run it.
Without an efficient means of re-trying queries and computational demand ever
increasing, hail may become prohibitively expensive for such projects.

To reduce the cost of re-tries, Hail experimented with the notion of "checkpoint
files" when writing a :scala:`MatrixTable` to persistent storage.:sup:`1`
A "checkpoint file" was a user-defined location where Hail accumulated metadata
about the partitions it had already computed and written.
Then, if a failure happened and the query was re-run, Hail would use these
metadata to only re-run those computations that had not previously succeeded,
accumulating the metadata from the successful runs into the checkpoint file.
This worked because query operations are (ostensibly) *referentially-transparent*,
ie. evaluation of an operation can be safely replaced with its value without
changing the meaning of the query.
There were a number of problems this design, including

* it only applied to writing :scala:`MatrixTable` and was not generalised to
  other kinds of hail expression (such as accumulations) on data types.
* the user was required to keep track of these staging files and guarantee that
  the :scala:`MatrixTable` contained therein was the same as that used in the
  query.
* it only applied to one write operation - queries could fail before they
  reached that write, in which case retrying would start from the beginning.

As part of Hail's continued effort to implement query execution on Hail batch,
`#12879 <https://github.com/hail-is/hail/pull/12879>`_ removed support for this
experimental feature for simplicity, presenting the opportunity to improve upon
checkpoint files.
We thus needs a means of resuming execution from where the last query left off.

:sup:`1` Not to be used with the `checkpoint` operation that writes then reads
an intermediate :scala:`Table`/:scala:`MatrixTable` computation to storage.

Proposed Change Specification
=============================

We propose to implement fast-restarts via "call-caching", a generalisation of
checkpoint files that acts on Hail query's intermediate representation
(:scala:`IR`), rather than higher-level abstractions exposed to the python DSL.
Experience tells us that the majority of time spent on expensive and
scientifically interesting queries is within the tasks generated by
:scala:`CollectDistributedArray` (:scala:`CDA`).
This is because many of the table operations in query's :scala:`IR` are lowered
into one or more :scala:`CDA` operations.
Consequently, we focus our attention on caching the intermediate results of
these tasks.
This has the benefit of not only fulfilling the purpose of checkpoint files for
:scala:`MatrixWrite` operations, but also any other operation that is lowered
in terms of :scala:`CDA`.

:scala:`CDA` can be thought of as a distributed map-reduce operation, from some
input "context" for each partition in a table (eg, the path to the file
where the partition is serialised), a computation on that partition, and some
combiner for the results of those computations.
For what follows, let an *activation* be a particular invocation of a
:scala:`CDA` pipeline (implemented via :scala:`collectDArray`).

At a high-level, when the driver performs an *activation*, it will look in its
*execution cache* to see if it had successfully performed that *activation*
in the past.
The *cache* contains the results for all the successful partition computations.
The driver compares the tasks for each partition with the results in the cache
and removes those tasks that have already been completed.
It then executes any remaining work and updates the execution cache with their
results.
If all the work completes successfully, the driver returns the now-cached
results to be used in the the rest of the query.
The driver will cache the results of successful *activations* only.
Failed *activations* (ie. those that errored) will be handled in the usual way,
potentially failing the query.

We require two things to determine if the driver had successfully executed an
operation:

1. a way of looking up *activations* in a *cache*, and
2. then design of the execution cache itself

Semantic Hashing
----------------
To lookup operations in the cache, we need a way of producing an identifier
that uniquely represents a particular *activation*.
We do this by defining a *semantic hash* for the activation, comprised of:

a) a *static* component computed from the :scala:`IR` that generated the
   operation
b) a *dynamic* component for the particular activation instance.

For most :scala:`IR` nodes, the *static* component can be computed purely from
their inputs plus some contribution uniquely representing the semantics of that
class of :scala:`IR`.
For :scala:`IR` nodes that read external files, we have to be a little more
cautious and ensure that those files haven't changed since we last read them.
Thus, we need to include some kind of checksum or digest of that file.
This static component can be passed down the lowering pipeline to the code
generator and driver, which, when performing an activation, can mix the static
component with a dynamically generated activation id to form the semantic hash.

Execution Cache
---------------

Users will "bring their own"\ :sup:`TM` cache directory where cached
computations will be stored.
This cache dir will be an prefix in local or cloud storage.
The driver will store cache files named ``{cachedir}/{semhash}``, where

- `cachdir` is a user-defined location, defaulting to
  `{tmp}/hail/{hail-pip-version}`
- `tmp` is either the local tempdir for spark and local backends, or the
  remote  tempdir for `QoB`.

These files will contain accumulated activation results, indexed by their
partition number.


> Discussion

* modify etag on cache objects when reading so their lifecycle policy is reset

etags don't seem to reset objects' retention though using an holds in
conjunction with bucket retention policies could!

cache thunks that validate files
clean cache on successful pipeline completion?

Examples
========

To opt in or out of fast-restarts, users will set hail flags in their python
client:

..  code-block:: python

    >> hl._set_flags(use_fast_restarts='1')
    >> hl._set_flags(cachedir='gs://my-bucket/cache/0')


Alternatively, users can set the corresponding environment variables at the
command line prior to starting their python session:

..  code-block:: sh

    >> HAIL_USE_FAST_RESTARTS=1 HAIL_CACHE_DIR='gs://my-bucket/cache/0' ipython

Notes:

- The definition of the ``cachedir`` does not imply
  ``use_fast_restarts``.
- If ``use_fast_restarts`` is defined, hail will write cache entries to
  a subfolder of the ``tmpdir`` by default.

Effect and Interactions
=======================


Your proposed change addresses the issues raised in the motivation. Explain how.

Also, discuss possibly contentious interactions with existing language or compiler
features. Complete this section with potential interactions raised
during the PR discussion.

Costs and Drawbacks
===================

.. Give an estimate on development and maintenance costs. List how this affects
.. learnability of the language for novice users. Define and list any remaining
.. drawbacks that cannot be resolved.

* Only cache around :scala:`CollectDistributedArray`
* Computing the semantic hash is a little tricky

  - Randomisation in queries will change the semantic hash, despite no changes
    in semantics
  - Such queries will not likely benefit from call-caching
  - eg. writing checkpoint files via the :code:`checkpoint()` operation

* Caching requires overhead from lookups and insertions
* Not completely hidden from user

  - writing state in a user-defined location exposes opportunities for failures
  - requires diligent error handling

* Requires that we start from the beginning until we get a cache-miss in a
  bottom-up execution strategy.

  - A more efficient fast-restart mechanism might search for the first
    cache-hit from the end of the query in a top-down execution strategy.

Unresolved Questions
====================

Execution Cache
---------------

* How long should the cache live?

  - Presumably as long as tmpdir as the files it caches reside in tmpdir.
  - Users can configure this by setting a lifetime policy on their bucket.

* Where do we write?

  - Configurable and user defined.
  - We'll likely default to the tempdir unless a user specifies otherwise.


* Who do we handle multiple processes executing the same query?

  - atomic writes, via db or file re-writes
  - one wins, doesn't matter which


Implementation Plan
===================

The reader should note that implementation examples below are for illustrative
purposes only and that the real implementation may differ slightly.

Semantic Hashes
---------------

Computing Static Component
^^^^^^^^^^^^^^^^^^^^^^^^^^

We can compute the static component of a semantic hash for the :code:`IR` in
a level-order traversal of the nodes in the :code:`IR`.
The particular ordering itself doesn't matter, only that an ordering is defined.
We also need to keep track of :code:`IR` shape when traversing;
it's possible to define two :code:`IR` trees with different shape but look
identical when flattened to a list.
We'll include an encoding of the node's trace (the path from the root node) to
account for this.

..  code-block:: scala

    def levelOrder(node: BaseIR): Iterator[(BaseIR, Trace)]


Since the ``IR`` contains references and compiler-generated names, we need to
normalise the names in the :code:`IR` (see :scala:`NormalizeNames.scala`)
to get consistent hashes.

The semantic hash is defined for the whole :code:`IR` (as apposed to prefixes
of the :code:`IR` tree, see Alternatives below).
Thus, we'll compute the hash as early as possible to minimise the computational
cost as the :scala:`IR` gets lowered and expanded.
This also reduces the number of :code:`BaseIR` operations we need to define
semantic hashes for (ie. only those that can be constructed in python).

Generally, a hash function takes a seed and some data (typically a stream of
numbers or bytes) and produces a hash.
That hash can be extended with more data by feeding it back to the hash function
as the seed.
What's needed is a way to encode the :code:`IR` as a byte stream.
A simple :code:`toString` is not sufficient as some nodes read data from
external blob-storage;
we need to ensure that the data hasn't changed since we last ran the query.
Furthermore, we can't define an encoding for some :code:`IR` nodes, so we need
a way to bail out:


..  code-block:: scala

    def encode(fs: FS, ir: BaseIR, trace: Trace): Option[Array[Byte]] = {
      val buffer =
        Array.newBuilder[Byte] ++= encodeTrace(trace)

      ir match {
        case Ref(name, _) =>
          buffer ++=
            encodeClass(classOf[Ref]) ++=
            name.getBytes

        case TableRead(_, _, reader) =>
          buffer ++=
            encodeClass(classOf[TableRead]) ++=
            encodeClass(reader.getClass)

          reader.pathsUsed.foreach { p =>
            // encode the contents of the file (md5 digest, etag, other)
            // to ensure it hasn't been modified since last time the query
            // was ran (if ever).
            buffer ++= encodeFile(fs, p)
          }

        case ir if DependsOnlyOnInputs(ir) =>
          buffer ++= encodeClass(ir.getClass)

        case _ if DontKnowHowToDefineSemhash(ir) =>
          return None

        case ... =>
      }

      Some(buffer)
    }


Then, assuming we have an appropriate hashing algorithm, seed and a way of
combining hashes, we can create and extend the hash with the contribution of
each node:

..  code-block:: scala

    var hash = Algorithm.SEED
    for ((node, trace) <- levelOrder(nameNormalizedIr)) {
      encode(fs, node, trace) match {
        case Some(bytes) => hash = Algorithm.extend(hash, bytes)
        case _           => return None
      }
    }
    Some(hash)


Observations:

- For all :code:`IR` nodes that depend only on their children and have no
  additional parameterisation, their semantic hash is simply some unique
  encoding for what that node means.
- Hashing :code:`IR`'s class is sufficient
- Note that the node's children will be hashed in the traversal
- There are times when we can't define a semantic hash (such as reading a
  table from a RVD). In these cases, we'll just return :scala:`None`.


Computing Dynamic Component
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The query driver is a single-threaded system that compiles and executes the
same queries in a repeatable way.
That is, if a query generates one or more :code:`CDA` nodes, those nodes will be
emitted in the same order.
This, we can use the static component in the same way as random number
generator state:

- When a :scala:`CDA` node is emitted, we can fork the semhash key-value
- We "mix" one value with the :code:`CDA`'s dynamic id to generate the semantic
  hash for that particular activation
- and update the static component state with the forked value for the next
  :code:`CDA` node.

To do this, we can add the function :code:`nextHash` to the
:code:`ExecuteContext` that returns a new `Hash` value to be mixed with the
dynamic component and updates internal state:

..  code-block:: scala

    final case class IrMetadata(semhash: Option[Int]) {
      private[this] var counter: Int = 0

      def nextHash: Option[Int] = {
        val bytes = intToBytes(counter)
        counter += 1
        semhash.map(Algorithm.extend(_, bytes))
      }
    }

Then, in :scala:`Emit.scala`:

..  code-block:: scala

    case cda: CollectDistributedArray =>
      ...
      semhash <- newLocal[Integer]("semhash")
      emitI(dynamicID).consume(
        ifMissing = nextHash.foreach { hash =>
          assign(semhash, boxToInteger(hash))
        },
        ifPresent = { dynamicID =>
          nextHash.foreach { staticHash =>
            val dynamicHash =
              invokeScalaObject(
                String.getClass,
                "getBytes",
                Array(classOf[String]),
                Array(dynamicID.loadString(cb))
              )

            val combined =
              invokeScalaObject(
                Algorithm.getClass,
                "extend",
                Array(classOf[Int], classOf[Array[Byte]]),
                Array(staticHash, dynamicHash)
              )

            assign(semhash, boxToInteger(combined))
          }
        }
      )

      // call `collectDArray` with semhash

Using :code:`Option` allows us to encode if we can compute a semantic hash
for the given :code:`IR`.
In the case when one cannot be computed, :code:`collectDArray` simply skips
reading and updating a cache.


Alternatives
^^^^^^^^^^^^

The following describes a means of computing and assigning the static portion of
semantic hashes for each node in the :code:`IR`.
The aim was to support extending queries that were developed incrementally and
interactively by recognising and caching query prefixes.
It does not work.
When the compiler sees a prefix of the query that it had already computed, it
would simply load the result from the cache rather than recompute.
In reality this is very hard to do as :code:`PruneDeadFields` changes the
semantics of the :code:`IR`, meaning the what's computed depends on how the
result is used.

We can compute the static component of a semantic hash from a bottom-up
traversal of the IR ``IR``.
Since the ``IR`` supports references, we need to compute a binding environment
top-down that maps names to their definitions, so we can look up the static
component of the value being referenced:

..  code-block:: scala

    type BindingEnv = Map[String, BaseIR]

    object FlattenTopDown {
      def apply(ir: BaseIR, env: BindingEnv): Iterator[(BaseIR, BindingEnv)] =
        ir match {
          case Let(name, value, body) =>
            FlattenTopDown(value, env) ++
            FlattenTopDown(body, env.put(name, value)) ++
            Iterator.single(ir, env)

          case ... =>
        }
    }

Then, assuming we have an appropriate hashing algorithm and a way of combining
a tree of hashes:

..  code-block:: scala

    def hash(a: Any): Hash = ???
    @newtype case class Hash(v: ???) {
      def <>(b: Hash): Hash = ???
    }

Then:

..  code-block:: scala

    object BottomUp {
      def apply(fs: FS, memo: Memo[Hash])(ir: BaseIR, env: BindingEnv): Hash =
        ir match {
          case Ref(name, _) =>
            hash(classOf[Ref]) <> memo(env(name))

          case TableRead(_, _, reader) =>
            reader
              .pathsUsed
              .map(fs.digest)
              .foldLeft(hash(classOf[TableRead]))(_ <> hash(_))

          case ir if DependsOnlyOnInputs(ir) =>
            ir.children.foldLeft(hash(ir.getClass))(_ <> memo(_))

          case ... =>
        }
    }

The binary combination of hashes makes it very hard to reason about the
likelihood of collisions.

Execution Cache
---------------

Given an interface for an :scala:`ExecutionCache`` of the form:

..  code-block:: scala

    trait ExecutionCache {
      def lookup(h: SemanticHash): Array[(Int, Array[Byte])]
      def put(h: SemanticHash, res: Array[(Int, Array[Byte])]): Unit
    }

We can implement a file-system cache that uses a file prefix plus the current
version of Hail to generate a "root" directory, under which all cache files are
stored by their semantic hash.

An implementation might look as follows:

..  code-block:: scala

    final case class FSExecutionCache(fs: FS, cachedir: String)
      extends ExecutionCache {

      override def lookup(h: SemanticHash): Array[(Int, Array[Byte])] =
        Using(fs.open(s"$cachedir/$h")) { _.split(newline).map(CacheLine.read) }
          .getOrElse(Array.empty)

      override def put(h: SemanticHash, res: Array[(Int, Array[Byte])]): Unit =
        fs.write(s"$cachedir/${HailContext.version}/$h") { ostream =>
          res.foreach { CacheLine.write(ostream) }
        }

        object CacheLine {
          def write(ostream: OutputStream): (Int, Array[Byte]) => Unit = {
            case (index, data) =>
              ostream.write(index)
              ostream.write(", ")
              ostream.write(Base64Encode(data))
              ostream.write(newline)
          }

          def read(s: String): (Int, Array[Byte]) = {
            val (index, s) = readInt(s)
            val (_, s) = readString(s, ", ")
            CacheLine(index, Base64Decode(s.getBytes))
          }
        }
    }

For testing, we can simply create a wrapper around a :scala:`mutable.HashMap`:

..  code-block:: scala

    @newtype case class MemExecutionCache(
        m: mutable.HashMap[SemanticHash, Array[(Int, Array[Byte])]]
    ) extends ExecutionCache { ... }

Endorsements
=============

.. (Optional) This section provides an opportunity for any third parties to express their
.. support for the proposal, and to say why they would like to see it adopted.
.. It is not mandatory for have any endorsements at all, but the more substantial
.. the proposal is, the more desirable it is to offer evidence that there is
.. significant demand from the community.  This section is one way to provide
.. such evidence.
